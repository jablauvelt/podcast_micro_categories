{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import util\n",
    "reload(util)\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from datetime import timedelta\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = True\n",
    "samp = '_samp' if samp else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Feature Names Shape:', (1192, 2))\n",
      "('Descriptions Shape:', (1192, 1))\n"
     ]
    }
   ],
   "source": [
    "# Load feature names\n",
    "fnames = np.array(pd.read_pickle('../interim/028_preproc_heavy_shows_concat' + samp + '.p'))\n",
    "\n",
    "# load concatenated descriptions\n",
    "desc = np.array(pd.read_pickle('../interim/028_preproc_heavy_show_description_concat' + samp + '.p'))\n",
    "\n",
    "print(\"Feature Names Shape:\", fnames.shape)\n",
    "print(\"Descriptions Shape:\", desc.shape)\n",
    "assert fnames.shape[0] == desc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicates\n",
      "\n",
      "x_reserve shape: (120,)\n",
      "y_reserve shape: (120,)\n",
      "\n",
      "x_dev shape: (268,)\n",
      "y_dev shape: (268,)\n",
      "\n",
      "x_train shape: (804,)\n",
      "y_train shape: (804,)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from full show list \n",
    "dups = []\n",
    "copy = []\n",
    "copy_desc=[]\n",
    "for i, j in zip(fnames.tolist(), desc.tolist()):\n",
    "    if i not in copy:\n",
    "        copy.append(i)\n",
    "        copy_desc.append(j)\n",
    "    else:\n",
    "        dups.append(i)\n",
    "fnames = np.asarray(copy)\n",
    "desc = np.asarray(desc)\n",
    "print \"Found %d duplicates\" % len(dups)\n",
    "\n",
    "# split data into reserve, dev, train\n",
    "x_reserve, y_reserve, x_train, y_train, x_dev, y_dev = util.random_data_split( desc, fnames)\n",
    "print \"\\nx_reserve shape:\", x_reserve.shape\n",
    "print \"y_reserve shape:\", y_reserve.shape\n",
    "print \"\\nx_dev shape:\", x_dev.shape\n",
    "print \"y_dev shape:\", y_dev.shape\n",
    "print \"\\nx_train shape:\", x_train.shape\n",
    "print \"y_train shape:\", y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Bag Of Words & KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with using CountVectorizer sparse array and KNN: 8.46% with k_neighbors = 1, F1 score: 9.73%\n",
      "Accuracy with using TfidfVectorizer sparse array and KNN: 12.94% with k_neighbors = 1, F1 score: 7.99%\n",
      "\n",
      "Time elapsed: 37.698047 second(s)\n"
     ]
    }
   ],
   "source": [
    "# function operates against dev and train data using a passed in vectorizer \n",
    "def vectorize(vec):\n",
    "    vectorizer_name = vec.__class__.__name__\n",
    "    x_train_counts = vec.fit_transform(x_train)\n",
    "    x_dev_vectors = vec.transform(x_dev)\n",
    "\n",
    "    best_score, best_param, f1_score = util.knn_test(x_train_counts, y_train, x_dev_vectors , y_dev)\n",
    "    print 'Accuracy with using %s sparse array and KNN: %3.2f%% with k_neighbors = %d, F1 score: %3.2f%%' % (vectorizer_name, best_score, best_param, f1_score )\n",
    "\n",
    "start = time.time()\n",
    "vectorize(CountVectorizer())\n",
    "vectorize(TfidfVectorizer())\n",
    "\n",
    "print \"\\n\", util.elapsed_time(start, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804, 50)\n",
      "(268, 50)\n",
      "Time elapsed: 02:27.581614 minute(s)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# treat each description as a document and generate a single vector for each\n",
    "def generate_document_vector_array( document ):\n",
    "    min_count = 2\n",
    "    size = 50\n",
    "    window = 4\n",
    "    document_vector_list = []\n",
    "\n",
    "    for i in range(document.shape[0]):\n",
    "        word_vecs = []\n",
    "        sentences = [sentence + '.' for sentence in  document[i].split('.')]\n",
    "        model = word2vec.Word2Vec(sentences, min_count=min_count, size=size, window=window)\n",
    "        for key in model.wv.vocab.keys():\n",
    "            word_vecs += model.wv[key].tolist()\n",
    "        data = np.asarray(word_vecs).reshape(-1, size)\n",
    "        document_vector = np.average(data, axis=0)\n",
    "        document_vector_list += document_vector.tolist()\n",
    "\n",
    "    return np.asarray(document_vector_list).reshape(-1, size)\n",
    "\n",
    "x_reserve_vectors = generate_document_vector_array(x_reserve)\n",
    "x_train_vectors = generate_document_vector_array(x_train)\n",
    "x_dev_vectors = generate_document_vector_array(x_dev)\n",
    "\n",
    "print 'x_reserve_vectors.shape', x_reserve_vectors.shape\n",
    "print 'x_train_vectors.shape', x_train_vectors.shape\n",
    "print 'x_dev_vectors.shape', x_dev_vectors.shape\n",
    "print elapsed_time(start, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy with KNNClassifier:\n",
    "start = time.time()\n",
    "best_score, best_param, f1_score = util.knn_test(x_train_vectors, y_train, x_dev_vectors, y_dev)\n",
    "print '\\nAccuracy with Word2Vec using using KNN: %3.2f%% with k_neighbors = %d, F1 score: %3.2f%%' % (best_score, best_param, f1_score )\n",
    "print elapsed_time(start, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy with Word2Vec using using LR: 2.35% with Cs = 1, F1 score: 0.09%\n",
      "Time elapsed: 03:53.483568 minute(s)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy with LinearRegression classiifier:\n",
    "reload(util)\n",
    "start = time.time()\n",
    "best_score, best_param, f1_score = util.lr_test(x_train_vectors, y_train, x_dev_vectors, y_dev)\n",
    "print '\\nAccuracy with Word2Vec using using LR: %3.2f%% with Cs = %d, F1 score: %3.2f%%' % (best_score, best_param, f1_score )\n",
    "print util.elapsed_time(start, time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Save To File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved arrays to file\n"
     ]
    }
   ],
   "source": [
    "x_reserve_vectors.dump('../interim/x_reserve_vectors' + samp + '.p')\n",
    "y_reserve.dump('../interim/y_reserve' + samp + '.p')\n",
    "x_train_vectors.dump('../interim/x_train_vectors' + samp + '.p')\n",
    "y_train.dump('../interim/y_train' + samp + '.p')\n",
    "x_dev_vectors.dump('../interim/x_dev_vectors' + samp + '.p')\n",
    "y_dev.dump('../interim/y_dev' + samp + '.p')\n",
    "print \"Saved arrays to file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
