{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import pickle\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"0.12\"))\n",
    "\n",
    "interim_path = '../../interim/'\n",
    "\n",
    "import rnnlm\n",
    "import util\n",
    "\n",
    "reload(rnnlm)\n",
    "reload(util)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = True\n",
    "samp = '_samp' if samp else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 3)\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data array\n",
    "with open( interim_path + 'preprocessed_concatenated_all' + samp + '.p') as p:\n",
    "\tpodcast_array = pickle.load(p)\n",
    "\n",
    "print podcast_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'dogs', 0.8680489659309387),\n",
       " (u'puppy', 0.8106428384780884),\n",
       " (u'pit_bull', 0.780396044254303),\n",
       " (u'pooch', 0.7627377510070801),\n",
       " (u'cat', 0.7609456777572632),\n",
       " (u'golden_retriever', 0.7500902414321899),\n",
       " (u'German_shepherd', 0.7465174198150635),\n",
       " (u'Rottweiler', 0.7437614798545837),\n",
       " (u'beagle', 0.7418621778488159),\n",
       " (u'pup', 0.740691065788269)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel = KeyedVectors.load_word2vec_format( interim_path + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "w2vmodel.most_similar('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w2vmodel.save('../interim/gensim_model')\n",
    "#w2vmodel = KeyedVectors.load('../interim/gensim_model')\n",
    "w2vmodel['dog'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALTERNATIVE 1\n",
    "# concatenate a maximum of 1000, 300-element word vectors per description\n",
    "# into a single 300k element document vector\n",
    "document_vectors = []\n",
    "for podcast in podcast_array:\n",
    "    # handle the edge case where there is no descrption\n",
    "    if len(podcast[1]) == 0:\n",
    "        podcast[1] = ''\n",
    "    doc_vec = {}\n",
    "    count = 0\n",
    "    for i in range(0, min(999, len(podcast[1]))):\n",
    "        word = podcast[1][i]\n",
    "        if word in w2vmodel:\n",
    "            if len(doc_vec) == 0:\n",
    "                doc_vec = w2vmodel[word]\n",
    "            else:\n",
    "                doc_vec = np.concatenate((doc_vec, w2vmodel[word]), axis=0)\n",
    "    document_vectors.append(doc_vec)\n",
    "\n",
    "# save the document vectors    \n",
    "filepath=interim_path + 'preprocessed_concatenated_document_vectors' + samp + '.p'\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump(document_vectors,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE 1 LOAD FROM FILE\n",
    "filepath = interim_path + 'preprocessed_concatenated_document_vectors' + samp + '.p'\n",
    "with open(filepath, 'rb') as f:\n",
    "    new_doc_vector_list = pickle.load(f)\n",
    "print len(new_doc_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaelrubin/anaconda2/lib/python2.7/site-packages/numpy/lib/function_base.py:1110: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/michaelrubin/anaconda2/lib/python2.7/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE 2\n",
    "# take the average of all of the word vectors for each description\n",
    "# treat each description as a document and generate a single vector for each\n",
    "# Training parameters\n",
    "size = 300\n",
    "document_vectors = []\n",
    "for podcast in podcast_array:\n",
    "    word_vecs=[]\n",
    "    for i in range(len(podcast[1])):\n",
    "        word = podcast[1][i]\n",
    "        if word in w2vmodel:\n",
    "            word_vecs += w2vmodel[word].tolist()\n",
    "    data = np.asarray(word_vecs).reshape(-1, size)\n",
    "    document_vectors.append(np.average(data, axis=0))\n",
    "\n",
    "# save the document vectors    \n",
    "filepath=interim_path + 'preprocessed_averaged_document_vectors' + samp + '.p'\n",
    "with open(filepath, 'wb') as f:\n",
    "    pickle.dump(document_vectors,f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE 2 LOAD FROM FILE\n",
    "filepath = interim_path + 'preprocessed_averaged_document_vectors' + samp + '.p'\n",
    "with open(filepath, 'rb') as f:\n",
    "    new_doc_vector_list = pickle.load(f)\n",
    "print len(new_doc_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(930, 300) (311, 300) (930, 1) (311, 1)\n"
     ]
    }
   ],
   "source": [
    "# break out train and test \n",
    "vl = np.array(new_doc_vector_list)\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(vl, podcast_array[:,[1]], test_size=0.25, random_state=42)\n",
    "print x_train.shape, x_dev.shape, y_train.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "max_time = 20\n",
    "batch_size = 10\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "V=len(new_doc_vector_list)\n",
    "H=len(new_doc_vector_list[0]) #*2\n",
    "model_params = dict(V=V, H=H, softmax_ns=200, num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = { lm.input_w_: w, \n",
    "                      lm.target_y_: y, \n",
    "                      lm.initial_h_ : h, \n",
    "                      lm.learning_rate_ : learning_rate, \n",
    "                      lm.use_dropout_ : use_dropout \n",
    "                    }\n",
    "        \n",
    "        h, cost, _ = session.run( [lm.final_h_, loss, train_op], feed_dict=feed_dict )\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d vectors at %d vps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=True, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 178]: seen 35800 vectors at 2386 vps, loss = 0.047\n",
      "[batch 353]: seen 70800 vectors at 2353 vps, loss = 0.024\n",
      "[batch 528]: seen 105800 vectors at 2343 vps, loss = 0.016\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[100] = -2147483648 is not in [0, 1241)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n\nCaused by op u'per_example_sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-3c4fc07c5ab8>\", line 12, in <module>\n    lm.BuildTrainGraph()\n  File \"rnnlm.py\", line 61, in wrapper\n    return function(self, *args, **kwargs)\n  File \"rnnlm.py\", line 229, in BuildTrainGraph\n    name=\"per_example_sampled_softmax_loss\")\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1412, in sampled_softmax_loss\n    name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1184, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 110, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1293, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[100] = -2147483648 is not in [0, 1241)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-3c4fc07c5ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         run_epoch(lm, session, bi,\n\u001b[1;32m     36\u001b[0m               \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m               tick_s=print_interval, learning_rate=learning_rate)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"[epoch %d] Completed in %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_timedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt0_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-8bb689a3b01b>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batch_iterator, train, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     }\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#### END(YOUR CODE) ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[100] = -2147483648 is not in [0, 1241)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n\nCaused by op u'per_example_sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-3c4fc07c5ab8>\", line 12, in <module>\n    lm.BuildTrainGraph()\n  File \"rnnlm.py\", line 61, in wrapper\n    return function(self, *args, **kwargs)\n  File \"rnnlm.py\", line 229, in BuildTrainGraph\n    name=\"per_example_sampled_softmax_loss\")\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1412, in sampled_softmax_loss\n    name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1184, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 110, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1293, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[100] = -2147483648 is not in [0, 1241)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(util)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 15\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = util.batch_generator(x_train, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        \n",
    "        run_epoch(lm, session, bi,\n",
    "              train=True, verbose=True,\n",
    "              tick_s=print_interval, learning_rate=learning_rate)\n",
    "        \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, util.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, x_train, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, x_dev, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, 0.0, 0)\n",
      "Score: 0.0399\n"
     ]
    }
   ],
   "source": [
    "reload(util)\n",
    "'''\n",
    "Function to compute precision, recall and f-score using knn and cosine similarity\n",
    "Finds the k nearest neighbors ...\n",
    "y_ is a vector of labels of length d\n",
    "x_arr is a matrix of vectors of length d, width v\n",
    "'''\n",
    "def scoreVectors(y_, x_arr):\n",
    "    k=10\n",
    "    y_pred=[]\n",
    "    y_true=[]\n",
    "    result=[]\n",
    "    for i in range(0, x_arr.shape[0]):\n",
    "        true_label = y_[i]\n",
    "        cnt = 0.0\n",
    "        found = 0.0\n",
    "        vec = x_arr[i]\n",
    "        r_idx, r_val = util.find_nn_cos(vec, x_arr, k)\n",
    "        \n",
    "        for j in range(0, r_idx.shape[0]):\n",
    "            if r_val[j] != 1.0:\n",
    "                cnt +=1\n",
    "                y_true+=true_label\n",
    "                if(y_[r_idx[j]]==true_label):\n",
    "                    y_pred+=y_[r_idx[j]]\n",
    "                    found+=1\n",
    "        if cnt > 0:\n",
    "            result.append(found / cnt)\n",
    "    score = np.average(result)\n",
    "    print precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print \"Score: %1.4f\" % score\n",
    "    return score\n",
    "            \n",
    "\n",
    "r  = scoreVectors( y_train, x_train )\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
