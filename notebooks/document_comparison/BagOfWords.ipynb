{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BagOfWords.ipynb\n",
    "Michael Rubin\n",
    "W266 Spring 2017\n",
    "\n",
    "Intention:\n",
    "Predict the class for a document based on word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "interim_path = '../../interim/'\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = False\n",
    "samp = '_samp' if samp else ''\n",
    "\n",
    "# pretty print elapsed time\n",
    "def elapsed_time(start, stop):\n",
    "    time_str = str(timedelta(seconds = time.time()-start))\n",
    "    if( time_str.startswith('0:')):\n",
    "        time_str = time_str[2:]\n",
    "        if( time_str.startswith('00:')):\n",
    "            time_str = time_str[3:]\n",
    "    return 'Time elapsed: %s' % time_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12407, 3)\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data array from file\n",
    "#   podcast+array[0]: podcast name\n",
    "#   podcast_array[1]: list of words in concatated descriptions, cleaned \n",
    "#   podcast_array[2]: subgenre (label)\n",
    "with open( interim_path + 'preprocessed_concatenated_all' + samp + '.p') as p:\n",
    "\tpodcast_array = pickle.load(p)\n",
    "\n",
    "print podcast_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 02:02.172438\n",
      "\n",
      "token_counts_by_class 841330\n",
      "token_counts 841330\n"
     ]
    }
   ],
   "source": [
    "# get the number of tokens plus counts for each subgenre \n",
    "token_counts_by_class = defaultdict(lambda: Counter())\n",
    "token_counts = Counter()\n",
    "start = time.time()\n",
    "subgenres = []\n",
    "for podcast in podcast_array:\n",
    "    document=podcast[1]\n",
    "    subgenre = podcast[2]\n",
    "    if subgenre not in subgenres:\n",
    "        subgenres.append(subgenre)\n",
    "    for word in document:\n",
    "        token_counts_by_class[word][subgenre]+=1\n",
    "        token_counts[word]+=1\n",
    "\n",
    "print elapsed_time(start, time.time())\n",
    "print''\n",
    "print \"token_counts_by_class\", len(token_counts_by_class)\n",
    "print \"token_counts\", len(token_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 46.517108\n",
      "token_log_probs_by_genre 841330\n"
     ]
    }
   ],
   "source": [
    "#log_probabilties of each token being in each subgenre\n",
    "token_log_probs_by_genre = {}\n",
    "start = time.time()\n",
    "for word in token_counts.keys():\n",
    "    # probability of the word occurring in the class is word_count_in_class / token_count_in_class\n",
    "    subgenre_probs={}\n",
    "    for subgenre in subgenres:\n",
    "        word_cnt = token_counts_by_class[word][subgenre]\n",
    "        token_cnt = token_counts[word]\n",
    "        log_prob=0.0\n",
    "        if token_cnt!=0:\n",
    "            log_prob = word_cnt/(token_cnt*1.0)\n",
    "\n",
    "        subgenre_probs[subgenre]=log_prob\n",
    "        token_log_probs_by_genre[word]=subgenre_probs\n",
    "\n",
    "print elapsed_time(start, time.time())\n",
    "print 'token_log_probs_by_genre', len(token_log_probs_by_genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 05.476432\n"
     ]
    }
   ],
   "source": [
    "# get the number of tokens plus their counts for each document\n",
    "token_counts_by_document_list=[]\n",
    "token_counts_by_document = Counter()\n",
    "start = time.time()\n",
    "for podcast in podcast_array:\n",
    "    document=podcast[1]\n",
    "    for word in document:\n",
    "        token_counts_by_document[word]+=1\n",
    "token_counts_by_document_list.append(token_counts_by_document)\n",
    "print elapsed_time(start, time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 28:56.107955\n",
      "Evaluating bag-of-words by word count:\n",
      "precision: 0.8746, recall: 0.7264, f-score: 0.7539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaelrubin/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "log_probs = []\n",
    "start = time.time()\n",
    "arr = podcast_array\n",
    "\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "\n",
    "for podcast in arr:\n",
    "    document=podcast[1]\n",
    "    subgenre_log_probs = {}\n",
    "    for word in document:\n",
    "        # compute the probability for the word being in each subgenre\n",
    "        for subgenre in subgenres:  \n",
    "            if subgenre not in subgenre_log_probs:\n",
    "                subgenre_log_probs[subgenre] = token_log_probs_by_genre[word][subgenre]\n",
    "            else:\n",
    "                subgenre_log_probs[subgenre] += token_log_probs_by_genre[word][subgenre]\n",
    "\n",
    "    max_log_prob=0.0\n",
    "    dominent_subgenre = ''\n",
    "    for subgenre in subgenre_log_probs.keys():\n",
    "        if subgenre_log_probs[subgenre] > max_log_prob:\n",
    "            max_log_prob = subgenre_log_probs[subgenre]\n",
    "            dominent_subgenre = subgenre\n",
    "            \n",
    "    y_true.append(podcast[2])\n",
    "    y_pred.append(dominent_subgenre)\n",
    "\n",
    "print elapsed_time(start, time.time())\n",
    "precision, recall, f_score, _ = precision_recall_fscore_support(y_true,y_pred,average='macro')\n",
    "print 'Evaluating bag-of-words by word count:'\n",
    "print 'precision: %3.4f, recall: %3.4f, f-score: %3.4f' % ( precision, recall, f_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
