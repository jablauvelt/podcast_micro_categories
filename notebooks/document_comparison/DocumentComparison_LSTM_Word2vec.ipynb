{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import pickle\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"0.12\"))\n",
    "\n",
    "interim_path = '../../interim/'\n",
    "\n",
    "import rnnlm\n",
    "import util\n",
    "\n",
    "reload(rnnlm)\n",
    "reload(util)\n",
    "\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = True\n",
    "load_word2vec=False\n",
    "samp = '_samp' if samp else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 3)\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data array\n",
    "#   podcast+array[0]: podcast name\n",
    "#   podcast_array[1]: list of words in concatated descriptions, cleaned \n",
    "#   podcast_array[2]: subgenre (label)\n",
    "with open( interim_path + 'preprocessed_concatenated_all' + samp + '.p') as p:\n",
    "\tpodcast_array = pickle.load(p)\n",
    "\n",
    "print podcast_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'dogs', 0.8680489659309387), (u'puppy', 0.8106428384780884), (u'pit_bull', 0.780396044254303), (u'pooch', 0.7627377510070801), (u'cat', 0.7609456777572632), (u'golden_retriever', 0.7500902414321899), (u'German_shepherd', 0.7465174198150635), (u'Rottweiler', 0.7437614798545837), (u'beagle', 0.7418621778488159), (u'pup', 0.740691065788269)]\n",
      "(300,)\n",
      "Time elapsed: 01:05.034479\n"
     ]
    }
   ],
   "source": [
    "if load_word2vec:\n",
    "    start = time.time()\n",
    "    w2vmodel = KeyedVectors.load_word2vec_format( interim_path + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    #w2vmodel.save('../interim/gensim_model')\n",
    "    #w2vmodel = KeyedVectors.load('../interim/gensim_model')\n",
    "    print w2vmodel.most_similar('dog')\n",
    "    print w2vmodel['dog'].shape \n",
    "    print util.elapsed_time(start, time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect the 300-element word vectors per description\n",
    "# into a single list and store in a file\n",
    "# 20 seconds with 16 CPUs\n",
    "# doesn't complete with 4 CPUs\n",
    "if load_word2vec:\n",
    "    start = time.time()\n",
    "    documents = [] # list of documents\n",
    "    for podcast in podcast_array:\n",
    "        word_vecs = [] # list of embedments for the words in the document\n",
    "        document = podcast[1]\n",
    "      \n",
    "        if document is not None:\n",
    "            for j in range(0, min(20000, len(document))):\n",
    "                word = document[j]\n",
    "                if word in w2vmodel:\n",
    "                    word_vecs.append(w2vmodel[word])\n",
    "\n",
    "        documents.append(np.array(word_vecs))\n",
    "        idx+=1\n",
    "    print util.elapsed_time(start, time.time())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using newly created list 1241\n"
     ]
    }
   ],
   "source": [
    "if load_word2vec:\n",
    "    # save the document vectors    \n",
    "    filepath=interim_path + 'preprocessed_concatenated_document_vector_arrays' + samp + '.p'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(documents,f)\n",
    "    new_doc_vector_list = documents\n",
    "    print \"using newly created list\", len(new_doc_vector_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list loaded from file 1241\n"
     ]
    }
   ],
   "source": [
    "if not load_word2vec:\n",
    "    filepath = interim_path + 'preprocessed_concatenated_document_vector_arrays' + samp + '.p'\n",
    "    with open(filepath, 'rb') as f:\n",
    "        new_doc_vector_list = pickle.load(f)\n",
    "    print \"list loaded from file\", len(new_doc_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(930,) (311,) (930, 1) (311, 1)\n"
     ]
    }
   ],
   "source": [
    "# break out train and test \n",
    "# x_train and x_dev are 1d arrays containing 2d arrays representing the word embedments for the document\n",
    "vl = np.array(new_doc_vector_list)\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(vl, podcast_array[:,[1]], test_size=0.25, random_state=42)\n",
    "print x_train.shape, x_dev.shape, y_train.shape, y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = { lm.input_w_: w, \n",
    "                      lm.target_y_: y, \n",
    "                      lm.initial_h_ : h, \n",
    "                      lm.learning_rate_ : learning_rate, \n",
    "                      lm.use_dropout_ : use_dropout \n",
    "                    }\n",
    "        \n",
    "        print 'feed_dict dump -- w.shape:', w.shape, 'y.shape:', y.shape, 'h.shape:', h[0][0].shape\n",
    "        \n",
    "        h, cost, _ = session.run( [lm.final_h_, loss, train_op], feed_dict=feed_dict )\n",
    "        print 'foo'\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d vectors at %d vps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=True, tick_s=3600)\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util' from 'util.pyc'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(util)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 600, 'num_layers': 1, 'softmax_ns': 200, 'V': 2543}\n",
      "[epoch 1] Starting epoch 1\n",
      "feed_dict dump -- w.shape: (50, 20) y.shape: (50, 20) h.shape: (50, 600)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[158] = -1 is not in [0, 2543)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n\nCaused by op u'per_example_sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-f69fc03f5e1e>\", line 31, in <module>\n    lm.BuildTrainGraph()\n  File \"rnnlm.py\", line 61, in wrapper\n    return function(self, *args, **kwargs)\n  File \"rnnlm.py\", line 229, in BuildTrainGraph\n    name=\"per_example_sampled_softmax_loss\")\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1412, in sampled_softmax_loss\n    name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1184, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 110, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1293, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[158] = -1 is not in [0, 2543)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f69fc03f5e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         run_epoch(lm, session, bi,\n\u001b[1;32m     55\u001b[0m               \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m               tick_s=print_interval, learning_rate=learning_rate)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"[epoch %d] Completed in %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty_timedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt0_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-2f7d248b35a3>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batch_iterator, train, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'feed_dict dump -- w.shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y.shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h.shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'foo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[158] = -1 is not in [0, 2543)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n\nCaused by op u'per_example_sampled_softmax_loss/embedding_lookup_1', defined at:\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-26-f69fc03f5e1e>\", line 31, in <module>\n    lm.BuildTrainGraph()\n  File \"rnnlm.py\", line 61, in wrapper\n    return function(self, *args, **kwargs)\n  File \"rnnlm.py\", line 229, in BuildTrainGraph\n    name=\"per_example_sampled_softmax_loss\")\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1412, in sampled_softmax_loss\n    name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 1184, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/embedding_ops.py\", line 110, in embedding_lookup\n    validate_indices=validate_indices)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1293, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/michaelrubin/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): indices[158] = -1 is not in [0, 2543)\n\t [[Node: per_example_sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, _class=[\"loc:@recurrent_layer/Variable\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](recurrent_layer/Variable/read, per_example_sampled_softmax_loss/concat)]]\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm)\n",
    "reload(util)\n",
    "\n",
    "x_data = x_train[0]\n",
    "\n",
    "# training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "V=len(x_data)\n",
    "H=len(x_data[0]) *2\n",
    "model_params = dict(V=V, H=H, softmax_ns=200, num_layers=1)\n",
    "\n",
    "print model_params\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 15\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = util.batch_generator(x_data, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        \n",
    "        run_epoch(lm, session, bi,\n",
    "              train=True, verbose=True,\n",
    "              tick_s=print_interval, learning_rate=learning_rate)\n",
    "        \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, util.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, x_train, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, x_dev, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
